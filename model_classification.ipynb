{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3UdoM22bJIqb",
        "wfwOokMWIqZJ",
        "HPRs7BxR79-8",
        "yTxP6PCqBKh2",
        "iGtF4WFyVoxF",
        "mVCLQvWMzWJ6",
        "fgZnD__399AZ"
      ],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN7rvoTgbz0Z704EoM0dbrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunokrp/ai-protectionism/blob/main/model_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASSIFICATION MODEL**"
      ],
      "metadata": {
        "id": "QMFjPO17VPMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SETTING UP ENVIROMENT**"
      ],
      "metadata": {
        "id": "P3ttsOj5I7cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Installing libraries, importing them, connecting to Google Drive and to Hugging Face**"
      ],
      "metadata": {
        "id": "Rl_A37Fcx-C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing libraries\n",
        "!pip install transformers datasets evaluate accelerate\n",
        "!pip install mapclassify"
      ],
      "metadata": {
        "id": "Myq7JHyfNJYJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import sklearn as skl\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "from mapclassify import classify\n",
        "import geopandas"
      ],
      "metadata": {
        "id": "mKMttAmqI7PR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging in Hugging Face\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "L4SnzB4bByhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/\"[Folder path here]\""
      ],
      "metadata": {
        "id": "wFPe-bPNJDor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PREPROCESSING DATA**"
      ],
      "metadata": {
        "id": "3UdoM22bJIqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading GTA dataset with descriptions\n",
        "df = pd.read_csv(\"interventions_with_descriptions_total.csv\", index_col=\"Unnamed: 0\")"
      ],
      "metadata": {
        "id": "RCEJOH27JP3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating copy to increase code reliability\n",
        "df_categories = df.copy()"
      ],
      "metadata": {
        "id": "IoNszTkI96bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming labels from string to numbers\n",
        "# Here I consider Green and Amber interventions as \"not protectionist\" and Red as \"protectionist\".\n",
        "def protectionist_label(description):\n",
        "  if description == 'Green':\n",
        "    return 0\n",
        "  elif description == 'Amber':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "df_categories['label'] = df_categories['Gta Evaluation'].apply(protectionist_label)"
      ],
      "metadata": {
        "id": "amyx9-fQJ_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking distribution of labels\n",
        "df_categories['label'].value_counts()"
      ],
      "metadata": {
        "id": "HiA1eBa4J_1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MODEL TRAINING**"
      ],
      "metadata": {
        "id": "wfwOokMWIqZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Splitting dataset into test and training data**"
      ],
      "metadata": {
        "id": "HPRs7BxR79-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing base dataset to contain only with description and label\n",
        "df_categories_clean = df_categories[['Description', 'label']]"
      ],
      "metadata": {
        "id": "cwmYvQ7CIw6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_categories_clean['Description'], df_categories_clean['label'], test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "FXsR00tqS_9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a unified training dataset, with descriptions and label values\n",
        "df_train_x = pd.DataFrame(X_train)\n",
        "df_train_y = pd.DataFrame(y_train)\n",
        "df_train = df_train_x.join(df_train_y).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Sj5KMRTuUUh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a unified testing dataset, with descriptions and label values\n",
        "df_test_x = pd.DataFrame(X_test)\n",
        "df_test_y = pd.DataFrame(y_test)\n",
        "df_test = df_test_x.join(df_test_y).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hlJdRR-XUUZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting DataFrame to Dataset, which is necessary for training\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Creating a DatasetDict, which is also necessary for training\n",
        "dataset_dict = DatasetDict({'train': train_dataset, 'test': test_dataset})"
      ],
      "metadata": {
        "id": "4xHlueXMUW9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Training**\n",
        "This following training code was based on https://huggingface.co/docs/transformers/en/tasks/sequence_classification\n",
        "\n",
        "*The classification model cannot be trained in the basic version of Google Colab due to RAM restrictions. I ran using the PRO version, with a T4-GPU and high-RAM enabled*."
      ],
      "metadata": {
        "id": "yTxP6PCqBKh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['Description'], truncation=True)\n",
        "\n",
        "# Creating tokenized dataframe\n",
        "tokenized_df = dataset_dict.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "EVdwUqDDLdj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data collator and creating a batch of examples\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "Db4bMQPnLiRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing F1 scoring evaluator\n",
        "import evaluate\n",
        "f1_score = evaluate.load(\"f1\")"
      ],
      "metadata": {
        "id": "cAlOocVvQIu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating function to compute metrics during training\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return f1_score.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "yCR5Ccu2QKVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping label to ID and ID to label. Positive is mapped to 1, which refers to the protectionist class\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "metadata": {
        "id": "EdW3MhUrQNDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pretrained DistilBERT model\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        "    )"
      ],
      "metadata": {
        "id": "wQeRYVheQUPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining training parameters and running trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"classification_model_protectionism\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_df[\"train\"],\n",
        "    eval_dataset=tokenized_df[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "M2D_PvYWQaG9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KWarNJFihXTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INFERENCE AND RESULTS**"
      ],
      "metadata": {
        "id": "iGtF4WFyVoxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Loading and preprocessing DPA dataset**"
      ],
      "metadata": {
        "id": "0Eu3dsru1SBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset with digital policies (DPA)\n",
        "%cd /content/drive/MyDrive/\"[Folder path with DPA descriptions]\"\n",
        "digital_policies_df = pd.read_csv(\"digital_policies.csv\")\n",
        "\n",
        "# Dropping duplicate interventions\n",
        "digital_policies_df.drop_duplicates(subset=['Latest Event Description'], inplace=True)\n",
        "\n",
        "# Creating dataframe only with AI-related interventions\n",
        "ai_policies_df = digital_policies_df[(digital_policies_df['Economic Activities'].str.contains('ML and AI development|Semiconductors|cloud computing') == True)].reset_index()\n",
        "\n",
        "# Exploding dataset to contain observations on the intervention-country level\n",
        "ai_policies_df_per_country = ai_policies_df.assign(countries=ai_policies_df['Implementing Countries'].str.split(', ')).explode('countries').reset_index(drop=True)\n",
        "\n",
        "# Loading dataset countries and regions\n",
        "cc_df = pd.read_csv('country_continent.csv')\n",
        "cc_df = cc_df.rename(columns={'country':'countries'})\n",
        "\n",
        "# Getting back to folder with model\n",
        "%cd /content/drive/MyDrive/\"AI INDUSTRIAL POLICY\"/\"SIPA_TEXT\"/\"GTA-MINING\""
      ],
      "metadata": {
        "id": "LRfyakns1OX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Inference**"
      ],
      "metadata": {
        "id": "0kxiF_05Bf6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading trained model\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"text-classification\", model=\"brunokrp/classification_model_protectionism\")"
      ],
      "metadata": {
        "id": "LAaUxkl14VX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions for each intervention description\n",
        "predictions = []\n",
        "\n",
        "for text in ai_policies_df_per_country[\"Latest Event Description\"]:\n",
        "  predictions.append(classifier(text))\n",
        "\n",
        "# Creating a dataframe with all predictions (contains the label and the probability)\n",
        "prediction_df = pd.DataFrame([item[0] for item in predictions])"
      ],
      "metadata": {
        "id": "juXMCkhv8_q3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining base dataframe with prediction dataframe, then merging it with country/region dataframe\n",
        "ai_policies_df_per_country_with_labels = ai_policies_df_per_country.join(prediction_df)\n",
        "ai_policies_df_per_country_with_labels = pd.merge(ai_policies_df_per_country_with_labels,cc_df,on='countries').drop_duplicates()\n",
        "\n",
        "# Replacing string labels with int labels\n",
        "ai_policies_df_per_country_with_labels[\"label\"] = ai_policies_df_per_country_with_labels[\"label\"].replace({'NEGATIVE': 0, 'POSITIVE': 1})\n",
        "\n",
        "# Because each intervention affects more than one country in each region,\n",
        "# Dropping duplicates to keep only unique intervention-region observations\n",
        "ai_policies_df_per_country_with_labels = ai_policies_df_per_country_with_labels.drop_duplicates(subset=[\"Policy Change ID\", \"region_1\"])\n",
        "\n",
        "# Keeping just two columns to facilitate visualization\n",
        "policies_per_region_label = ai_policies_df_per_country_with_labels[[\"region_1\", \"label\"]]\n",
        "\n",
        "# Grouping dataframe by region\n",
        "# Getting average of label values\n",
        "# Getting count of interventions\n",
        "policies_per_region_label = policies_per_region_label.groupby(['region_1']).agg(['count','mean']).reset_index()\n",
        "\n",
        "# Removing Multi-Index created by the groupby function using agg\n",
        "policies_per_region_label.columns = ['_'.join(col) for col in policies_per_region_label.columns]"
      ],
      "metadata": {
        "id": "FZJr1Z3Zbrvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Plotting in map**"
      ],
      "metadata": {
        "id": "4Sz_WZE9kYwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting shapefile from world regions [data extracted from https://hub.arcgis.com/datasets/a79a3e4dc55343b08543b1b6133bfb90/explore]\n",
        "%cd /content/drive/MyDrive/\"[Folder with world regions data]\"\n",
        "gdf = gpd.read_file(\"World_Regions.shp\")\n",
        "\n",
        "# Renaming columns to facilitate join and replacing two regions values to ensure compatibility between dataframes\n",
        "gdf = gdf.rename(columns={'REGION':'region_1_'})\n",
        "gdf.region_1_.replace(\"Australia/New Zealand\", \"Australia and New Zealand\", inplace=True)\n",
        "gdf.region_1_.replace(\"Southeastern Asia\", \"South-eastern Asia\", inplace=True)"
      ],
      "metadata": {
        "id": "MAsGGLpHf0y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging dataframe that contains predictions with shapefiles\n",
        "policies_per_region_shapefile = pd.merge(policies_per_region_label,gdf,on='region_1_')\n",
        "\n",
        "# Increasing value of each region by 0.001 to differentiate between regions without policies and regions with no protectionist policies\n",
        "policies_per_region_shapefile[\"label_mean\"] = policies_per_region_shapefile[\"label_mean\"] + 0.001\n",
        "\n",
        "# Transforming dataframe into a GeoDataFrame to visualize information in a map\n",
        "gdf_policies = geopandas.GeoDataFrame(policies_per_region_shapefile)"
      ],
      "metadata": {
        "id": "qIFqv7Zski1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining map parameters\n",
        "\n",
        "m = gdf_policies.explore(\n",
        "  column = 'label_mean',\n",
        "  tooltip = ['region_1_', 'label_mean', 'label_count'],\n",
        "  cmap = 'YlOrRd',\n",
        "  legend = True,\n",
        "  popup = True\n",
        ")"
      ],
      "metadata": {
        "id": "zBWZm5n6pAem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting map\n",
        "m"
      ],
      "metadata": {
        "id": "-Jd8a3inpG6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Shap Values**\n",
        "Based on SHAP documentation: https://shap.readthedocs.io/en/latest/example_notebooks/text_examples/sentiment_analysis/Positive%20vs.%20Negative%20Sentiment%20Classification.html"
      ],
      "metadata": {
        "id": "mVCLQvWMzWJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataframe only with interventions classified as protectionist with high probability\n",
        "positive_policies = ai_policies_df_per_country_with_labels[(ai_policies_df_per_country_with_labels[\"score\"]>0.999) & (ai_policies_df_per_country_with_labels[\"label\"]==1)].drop_duplicates(subset=[\"Latest Event Description\"])"
      ],
      "metadata": {
        "id": "U1flAyCY8eWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"brunokrp/classification_model_protectionism\")\n",
        "\n",
        "# Loading explainer model\n",
        "explainer = shap.Explainer(classifier)\n",
        "\n",
        "# Getting shap values from a random sample of 50 interventions\n",
        "# Chose a sample because this process is time and computing intensive\n",
        "shap_values = explainer(positive_policies[\"Latest Event Description\"].sample(n=50).to_list())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5cq7AAVy2lt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting text explainer for the sample interventions\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "nDXYOJvC9X6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}